---
title: "Hw2_q11.Rmd"
author: "Muhammad Ahmed Chaudhry"
date: "5/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(dplyr)
library(gbm)

```


```{r}
# Reading in the dataset

# Training Data
spam_train_df<- read.csv("spam_stats315B_train.csv", header = FALSE)

# Test Data
spam_test_df<-read.csv("spam_stats315B_test.csv", header=FALSE)


# basic data pre-processing
spam_train_df <- data.frame(sapply(spam_train_df, as.numeric))
spam_test_df <- data.frame(sapply(spam_test_df, as.numeric))


rflabs<-c("make", "address", "all", "3d", "our", "over", "remove","internet",
          "order", "mail", "receive", "will","people", "report", "addresses","free", 
          "business","email", "you", "credit", "your", "font","000","money","hp", "hpl", 
          "george", "650", "lab", "labs","telnet", "857", "data", "415", "85", "technology", 
          "1999","parts","pm", "direct", "cs", "meeting", "original", "project","re","edu", 
          "table", "conference", ";", "(", "[", "!", "$", "#","CAPAVE", "CAPMAX", "CAPTOT","type")
colnames(spam_train_df)<-rflabs
colnames(spam_test_df)<-rflabs

```


```{r}
# split the dataset
set.seed(131)
train_rows <- sample(1:nrow(spam_train_df), size = as.integer(nrow(spam_train_df) * 0.7)) # 70% training
val_rows <- setdiff(1:nrow(spam_train_df), train_rows)

# creating training and validation sets
train_data <- spam_train_df[train_rows,]
val_data <- spam_train_df[val_rows,]

# Fitting the gbm model
gbm0<-gbm(type~.,data=train_data,train.fraction=0.8,
          interaction.depth=4,shrinkage=.05,
          n.trees=2500,bag.fraction=0.5,
          cv.folds=5,distribution="bernoulli",verbose=F, n.cores=1)

# Getting the best iteration and prediction from the best iteration
best.iter <- gbm.perf(gbm0,method="cv")
gbm0.predict<-predict(gbm0,val_data,type="response",n.trees=best.iter)

# Setting the threshold for classification at 0.5
thresh <- 0.5
yhat <- sapply(gbm0.predict, FUN=function(x) if (x > thresh) 1 else 0)
ytrue <- val_data$type

# Getting the misclassification rate
misclass <- mean(yhat != ytrue)
misclass

# Predicting on the test set
gbm0.predict_test<-predict(gbm0,spam_test_df,type="response",n.trees=best.iter)
thresh <- 0.5
yhat_test <- sapply(gbm0.predict_test, FUN=function(x) if (x > thresh) 1 else 0)
ytrue_test <- spam_test_df$type
misclass <- mean(yhat_test != ytrue_test)
misclass

# Test set misclassification of spam emails
misclass.spam <- mean(yhat_test[which(ytrue_test == 1)] != ytrue_test[which(ytrue_test == 1)])
misclass.spam

# Test set misclassification of non-spam emails
misclass.notspam <- mean(yhat_test[which(ytrue_test == 0)] != ytrue_test[which(ytrue_test == 0)])
misclass.notspam
```

## Part a)
With a split of 70% of the spam_stats315B_train.csv for training and 30% for validation (we do not perform hyperparameter tuning over a grid of parameters however, use the validation set to obtain an estimate of the misclassification rate), we get an estimate of $4.67 \%$.  The test set misclassification rate obtained is $4.04 \%$. Within the test set, of all the spam emails, $4.69%$ were misclassified, whereas, of all non-spam or "good" emails, $3.60\%$ were misclassified. 


## Part b)
We want to lower the non-spam misclassification rate to be less than $0.3\%$. Since the gbm package and function do not allow us to modify the cost matrix directly, we try a few combinations of different threshold values (other than 0.5) for classification, as well as different weights for the spam and non-spam emails to achieve the required rate. If we simply modify the threshold value, it was observed that having a threshold of $0.983$ was the smallest threshold value that gave us a non-spam misclassification rate of less than $0.3\%$. However, with this approach, we got the overall misclassification rate to be $15.45\%$. The exact misclassification rate for non-spam email was $0.22\%$, and the misclassification rate for spam emails was $38.03\%$. So then we wrote a function to try a different threshold value but also a series of different weight values for spam and non-spam emails to reach the required non-spam misclassification rate while trying to keep the overall misclassification rate low. With the smallest value of the weight of $400$ for non-spam and $10$ for spam, and a threshold value of $0.75$, we got a non-spam misclassification rate of $0.22\%$, a spam misclassification rate of $14.47\%$, and an overall misclassification rate of $35.60 \%$. We went ahead with this model. With regards to the important variables for discriminating good emails from spam emails, the five variables with the highest relative influence values are "$", "!", "remove", "hp", and "free", which make intuitive sense. To see the dependence of the response on the two most important variables "$" and $!$, we created a partial dependence plot and see that there is indeed a strong interaction between "$" and "!". 


```{r}
# (Naive threshold approach)
misclass.notspam_series <- c()
misclass.spam_series <- c()
for (thresh in seq(0.98, 0.99, 0.001)) {
  yhat_test <- sapply(gbm0.predict_test, FUN=function(x) if (x > thresh) 1 else 0)
  misclass.spam <- mean(yhat_test[which(ytrue_test == 1)] != ytrue_test[which(ytrue_test == 1)])
  misclass.notspam <- mean(yhat_test[which(ytrue_test == 0)] != ytrue_test[which(ytrue_test == 0)])
  misclass.notspam_series <- c(misclass.notspam_series, misclass.notspam)
  misclass.spam_series <- c(misclass.spam_series, misclass.spam)
}
misclass.spam_series
misclass.notspam_series

# Originally threshold was found to be 0.988
thresh <- 0.983
yhat_test <- sapply(gbm0.predict_test, FUN=function(x) if (x > thresh) 1 else 0)
ytrue_test <- spam_test_df$type
misclass <- mean(yhat_test != ytrue_test)
misclass
# The overall misclassification of this naive case is 15.45%
misclass.spam <- mean(yhat_test[which(ytrue_test == 1)] != ytrue_test[which(ytrue_test == 1)])
misclass.spam
misclass.notspam <- mean(yhat_test[which(ytrue_test == 0)] != ytrue_test[which(ytrue_test == 0)])
misclass.notspam
# We misclassify 38% of spam emails and only 0.22% of non.spam emails. 

```

```{r}

# Function to try different weight values with a given threshold

spam.pred.test <- function(train_data, test_data, weights, thresh, use_best = FALSE) {
  #' @param train_data Data frame used to fit model
  #' @param test_data Data frame used to evaluate model
  #' @param weights Weight values given to training data observations (must have same length)
  #' @param thresh  Threshold used to evaluate output probabilities as spam, not spam
  #' @param use_best Use the optimal number of iterations by gbm.perf (cv)

  gbm1<-gbm(type~.,data=train_data,
            train.fraction=0.8,
            weights=weights,
            interaction.depth=4,shrinkage=.05,
            n.trees=2500,bag.fraction=0.5,
            cv.folds=5,distribution="bernoulli",verbose=T,n.cores=1)
  gbm1.predict <- NULL
  if (use_best) {
    best.iter <- gbm.perf(gbm1,method="cv")
    gbm1.predict<-predict(gbm1,test_data,type="response",n.trees=best.iter)
  }
  else {
    gbm1.predict<-predict(gbm1,test_data,type="response",n.trees=300)
  }
  yhat <- sapply(gbm1.predict, FUN=function(x) if (x > thresh) 1 else 0)
  ytrue <- test_data$type
  misclass <- mean(yhat != ytrue)
  misclass.spam <- mean(yhat[which(ytrue == 1)] != ytrue_test[which(ytrue == 1)])
  misclass.notspam <- mean(yhat[which(ytrue == 0)] != ytrue[which(ytrue == 0)])
  list(misclass.overall = misclass, 
       misclass.spam = misclass.spam, 
       misclass.notspam = misclass.notspam)
}


```


```{r}

misclass_weighted.overall <- c()
misclass_weighted.notspam <- c()
for (w in seq(100, 600, 100)) {
  print(w)
  weights <- rep(1.0, nrow(spam_train_df))
  weights[which(spam_train_df$type == 0)] = w # upweight non-spam
  weights[which(spam_train_df$type == 1)] = 10
  
  set.seed(2021)
  errors <- spam.pred.test(spam_train_df, spam_test_df, weights, thresh=0.75)
  misclass_w <- errors$misclass.overall
  misclass.notspam_w <- errors$misclass.notspam
  misclass_weighted.overall <- c(misclass_weighted.overall, misclass_w)
  misclass_weighted.notspam <- c(misclass_weighted.notspam, misclass.notspam_w)
}
misclass_weighted.notspam
misclass_weighted.overall 
```

```{r}

# Applying the weights value obtained that gets us the required non-spam misclassification rate

weights[which(spam_train_df$type == 0)] = 400 # upweight non-spam
weights[which(spam_train_df$type == 1)] = 10

set.seed(2021)
errors <- spam.pred.test(spam_train_df, spam_test_df, weights, thresh=0.75, use_best=FALSE)
errors$misclass.overall
errors$misclass.spam
errors$misclass.notspam
```


```{r}
summary(gbm1,main="RELATIVE INFLUENCE OF ALL PREDICTORS")
```


```{r}
# 53 and 52 correspond to the two variables with the highest relative influence values

plot(x=gbm1,c(53,52),best.iter,ylim=c(0,3),main="Partial Dependence on ’$’ and ’!’ ")
```

